---
title: 【机器学习】：支持向量机
date: 2023-02-10 14:37:16
categories: AI模块
tags:
    - 人工智能
    - 算法
    - 机器学习
    - 编程语言
    - 啃书
layout: tweet
icon: arrow-forward-outline
---

# **支持向量机**

支持向量机在我的数学建模中已经使用过多次，因此在这里我不写出代码，只给出当时比赛的几个pdf。（懒狗一只）

## **基本内容**

支持向量机的三个重点内容：间隔、对偶、核方法。

有了这三个重点属性，我们可以很好的构建适合数据的超平面，使得模型具有较高的准确性和普适性。

<!-- more -->

## **间隔与支持向量**

在样本空间中，划分超平面可以通过$\omega^Tx+b=0$这样的线性方程来描述，其中$\omega$为法向量。

分类结果可以划分为两个区域，可以用如下形式表示：
$$
\omega^Tx+b\ge+1\newline
\omega^Tx+b\le-1
$$
两个区域的距离与这两个平面的距离相等，距离之和为：$\gamma=\frac{2}{||\omega||}$，这个值被称为间隔。

想要最大化间隔，相当于最小化$||\omega||^2$，这也是支持向量机基本型的损失函数。

## **对偶问题**

对每个约束添加拉格朗日乘子$\alpha_i\ge0$，则该问题可以使用拉个朗日乘子法进行计算，得到对偶问题：
$$
max(\Sigma_{i=1}^m\alpha_i-\frac{1}{2}\Sigma_{i=1}^m\Sigma_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j)
$$
解出$\alpha$后，即可求出$\omega$与$b$，进而建立模型：
$$
f(x)=\omega^Tx+b=\Sigma_{i=1}^m\alpha_iy_ix_i^Tx+b
$$

## **核函数与核方法**

在实际应用中，很多训练样本并不满足线性可分这一要求，这个时候，我们应当选用合适的核函数将训练样本映射到更高的维度，在高维度的空间中寻找到对应的超平面，令$\Phi(x)$为$x$在核函数映射后的特征向量，在特征空间中的超平面模型应当变为：
$$
f(x)=\omega^T\Phi(x)+b
$$
常见的核函数可以参考$CSDN$。

## **软间隔与正则化**

在实际生活中，很难做到找到某个核函数使得训练集恰好在特征空间中完全线性可分，也很难断定这个线性可分的结果是不是由于过拟合造成的，为了缓解这个问题，我们需要允许支持向量机在一些样本上出错。为此，我们引入软间隔的概念。

1. 硬间隔：所有样本必须划分正确
2. 软间隔：允许部分样本划分错误

于是，优化目标可以写成：
$$
min(\frac{1}{2}||\omega||^2+C\Sigma_{i=1}^ml_{0/1}(y_i(\omega^Tx_i+b)-1))
$$
其中$l_{0/1}$为"$0/1$损失函数"（可替换为其他函数），$C>0$为常数。

显然，当C趋近无穷大时，迫使所有样本满足约束；当$C$取有限值时，允许一些样本不满足约束。

更一般的形式可以写成：
$$
min_f(\Omega(f)+C\Sigma_{i=1}^ml(f(x_i),y_i))
$$

## **蒟蒻的论文**

2022年第三届“华数杯”全国大学生数学建模竞赛（国三等奖）：

{% pdf ./2022年第三届“华数杯”全国大学生数学建模竞赛（国三等奖）.pdf %}

2022年高教社杯数学建模竞赛（省二等奖）：

{% pdf ./2022年高教社杯数学建模竞赛（省二等奖）.pdf %}
