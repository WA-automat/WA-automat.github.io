---
title: 卷积神经网络
date: 2023-03-06 15:18:52
categories: AI模块
tags:
    - 人工智能
    - 算法
    - 深度学习
    - 编程语言
    - 啃书
sticky: 120
cover: /2023/03/06/卷积神经网络/LeNet简化.png
photo: /2023/03/06/卷积神经网络/LeNet.png
---

最近一段时间一直在写$icode$比赛的小程序，准备实现一个小狗收养的系统。并决定在其中添加一个拍照识别小狗类别的模块。

初定是使用$ResNet50$神经网络，由于最近一直在编写这个项目，因此一直没有更新深度学习的博客，现在决定来填填坑。

本篇博客主要内容是：
1. 介绍全连接层与卷积层之间的关系
2. 介绍卷积层的概念与公式
3. 讲解卷积层在图像中的应用以及一些问题的解决方案
4. 解释汇聚层的原理与分类

<!-- more -->

## **从全连接层到卷积**

卷积神经网络基于空间的不变性这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。

1. **平移不变性**：对不同位置具有相似的反应；
2. **局部性**：只探索输入图像中的局部区域。

**$$
[H]_{i,j}\\
=[U]_{i,j}+\Sigma_{k}\Sigma_{l}[W]_{i,j,k,l}[X]_{k,l}\\
=[U]_{i,j}+\Sigma_{k}\Sigma_{l}[V]_{i,j,a,b}[X]_{i+a,j+b}
$$**

**平移不变性**：

$$
[H]_{i,j}=u+\Sigma_a\Sigma_b[V]_{a,b}[X]_{i+a,j+b}
$$

**局部性**：

$$
[H]_{i,j}=u+\Sigma_{a=-\Delta}^{\Delta}\Sigma_{b=-\Delta}^{\Delta}[V]_{a,b}[X]_{i+a,j+b}
$$

其中，$V$被称为卷积核。

---

## **卷积**

两个函数$f$，$g$的卷积被定义为：

$$
(f*g)(x)=\int f(z)g(x-z)dz
$$

对于二维张量，卷积为：

$$
(f*g)(i,j)=\Sigma_a\Sigma_bf(a,b)g(i-a,j-b)
$$

卷积层的本质其实是互相关运算，是通过输入张量与核张量通过互相关运算生成输出张量。

输出大小等于输入大小$n_h \times n_w$减去卷积核大小$k_h \times k_w$，即：$(n_h - k_h + 1) \times (n_w - k_w + 1)$

输出的卷积层有时被称为特征映射，因为它可以被视为一个输入映射到下一层的空间维度的转换器。在卷积神经网络中，对于某一层的任意元素$x$，其感受野是指在前向传播期间，可能影响$x$计算的所有元素。

## **图像**

图像本质其实并不是一个二维张量，而是一个由高度、宽度、颜色组成的三维张量。

**填充**：处理边缘像素丢失的方法

在输入图像的边缘填充元素（通常我们的填充元素是0），防止在经过多次连续的卷积时，边缘丢失的像素数过多。

**步幅**：高效计算的方法

在计算互相关时，卷积窗口从输入张量的左上角开始向下、向右滑动。

## **多输入多输出通道**

输入包含多个通道时，可以构造一个具有与输入数据相同输入通道数的卷积核，以便与输入数据进行互相关运算。

希望输出包含多个通道时，可以设置多组卷积核，一组卷积核得到的结果作为一个输出通道

**$1 \times 1$卷积核**：本质上是在通道之间的计算。

## **汇聚层**

### **最大汇聚与平均汇聚**

最大汇聚：计算汇聚窗口中的最大值

平均汇聚：计算汇聚窗口中的平均值

同样的，汇聚层也可以调整填充和步幅
