{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 数据加载器\n",
    "在前面的线性回归模型中，我们使用的数据很少，所以直接把全部数据放到模型中去使用。但是在深度学习中，数据量通常是都非常多，非常大的，如此大量的数据，不可能一次性的在模型中进行向前的计算和反向传播，经常我们会对整个数据进行随机的打乱顺序，把数据处理成个个的batch,同时还会对数据进行预处理。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据集类：Dataset\n",
    "需要自行实现两个方法：__len__(self)与__getitem__(self, index)\n",
    "分别表示元素个数与索引"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 迭代数据集：DataLoader\n",
    "使用上述的方法能够进行数据的读取，但是其中还有很多内容没有实现：\n",
    "\n",
    "1. 批处理数据（Batching the data）\n",
    "2. 打乱数据（Shuffling the data）\n",
    "3. 使用多线程multiprocessing并行加载数据\n",
    "\n",
    "DataLoader参数的含义：\n",
    "\n",
    "1. dataset：提前定义的dataset的实例；\n",
    "2. batch_size：传入数据的batch大小，常常是32、64、128、256’\n",
    "3. shuffle：bool类型，表示是否在每次获取数据的时候提前打乱数据；\n",
    "4. num_workers：加载数据的线程数。\n",
    "5. drop_last：bool类型，为真，表示最后的数据不足一个batch，就删掉"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 完成数据集类\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" 必须实现，作用是:获取索引对应位置的一条数据 :param index: :return: \"\"\"\n",
    "        return MyDataset.to_tensor(self.data[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" 必须实现，作用是得到数据集的大小 :return: \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_tensor(data):\n",
    "        \"\"\" 将ndarray转换成tensor :param data: :return: \"\"\"\n",
    "        return torch.from_numpy(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data = MyDataset()  # 实例化对象\n",
    "print(data[0])  # 取第1条数据\n",
    "print(len(data))  # 获取长度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=data, batch_size=2, shuffle=True, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 总结\n",
    "1. Dataset是一个抽象类，需要派生一个子类构造数据集，需要改写的方法有__init__，__getitem__等。\n",
    "2. DataLoader是一个迭代器，方便我们访问Dataset里的对象。\n",
    "3. 数据和标签是tuple元组的形式，使用Dataloader然后使用enumerate函数访问它们。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}